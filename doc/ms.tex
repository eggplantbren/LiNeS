\documentclass[a4paper, 12pt]{article}
\usepackage{amsmath}
\usepackage{dsfont}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage[left=2cm, right=2cm, bottom=3cm, top=2cm]{geometry}
\usepackage{natbib}
\usepackage{microtype}

\title{LiNeS: Linked Nested Sampling}
\author{Brendon J. Brewer}
\date{}

\begin{document}
\maketitle

\abstract{\noindent Abstract}

% Need this after the abstract
\setlength{\parindent}{0pt}
\setlength{\parskip}{8pt}

\section{Nested Sampling}
Nested Sampling \citep{skilling2006nested}
is a computational method for calculating normalising
constants of probability distributions. In most applications, this is
the marginal likelihood or `evidence' from Bayesian inference, which is
the expected value of the likelihood with respect to the prior distribution
for the parameters
\begin{align}
p(D | M) &= \int p(\theta | M) p(D | \theta, M) \, d\theta
\end{align}
where $M$ is a set of model assumptions, $\theta$ is a collection of
parameters, and $D$ is the data.

In more computationally-oriented notation, this may be written
\begin{align}
Z &= \int \pi(\theta)L(\theta) \, d\theta
\end{align}
where $\pi(\theta)$ is the prior distribution and $L(\theta)$ is the
likelihood function.

%The constrained prior corresponding
%to level $j$ is
%\begin{align}
%p_j(\theta) &\propto
%\frac{\pi(\theta)\mathds{1}\left[L(\theta) > L_j\right]}
%{X_j}.
%\end{align}

%The LIS procedure, when applied to the NS sequence of
%distributions, samples a probability distribution over
%$\theta_{j,i}, K_j$ where $j$ indexes the level,
%$i \in \{1, 2, ..., N\}$ is the
%iteration within the level, and $K$ is a discrete
%selection variable (one per level). The distribution for the
%first level's quantities is
%\begin{align}
%p_{\rm LIS}\left(K_0, \left\{\theta_{0,i}\right\}\right)
%&= \frac{1}{N} \prod_{i=K_0+1}^N T(\theta_{0,i} | \theta_{0,i-1})
%\end{align}


\section{Comparison with DNest4}

\subsection{SpikeSlab problem, 20 dimensions, peak at 0.031}
In DNest4, I used this OPTIONS file (the current standard):
\begin{align}
\{\texttt{5, 10000, 10000, 100, 100, 10, 100, 10000}\}
\end{align}
and used
a single thread. This is $10^8$ likelihood evaluations.

Results:
Done 108 runs. RMS error = 0.3331568245987485

In LiNeS, I used 300 particles and 500 MCMC steps per iteration
for the Classic warmup run, and
LNS runs of 10,000 steps each, until $10^8$ MCMC steps
had been done (not counting the classic warmup).
Consider this the standard from which everything else
departs.

Results:
Done 147 runs. RMS error = 0.47972851663867766

What if, in LNS we trade some MCMC steps per level
for more repetitions? Changing to LNS runs of
3,000 steps gives:
Done 102 runs. RMS error = 0.6768875559288647
So no, that doesn't help.

Changing to LNS runs of 30,000 steps gives:
Done 179 runs. RMS error = 0.41877131444758076

Clanging to LNS runs of 100,000 steps gives:
Done 66 runs. RMS error = 0.43008576478784943

Does LiNeS performance depend strongly on the quality of the
classic warmup run? To test, I changed the classic warmup run to 5000
particles. Did it make a difference? Nup. Here's the result:
Done 101 runs. RMS error = 0.4770324559809218

{\bf DNest4 seems to be unequivocally better on this problem.}

\subsection{SpikeSlab problem, 20 dimensions, peak at 0.0}
As above, the only change being the SpikeSlab definition, making it
easier.
DNest4: Done 321 runs. RMS error = 0.04529294847033139
LiNeS: Done 468 runs. RMS error = 0.05811019003556211


\bibliographystyle{chicago}
\bibliography{references}

\end{document}

